{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "Summary: \n",
    "- Separate train, val, test filepath in order to create neccessary FiftyOne dataset\n",
    "- Extract label for each class_id for better data exploration (i.e. True breed name instead of numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'annotations']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "m_path = '/home/tungnguyendinh/.fastai/data/oxford-iiit-pet'\n",
    "print(os.listdir(m_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "df = pd.read_csv(f\"{m_path}/annotations/list.txt\", sep = \" \", header=None, skiprows=6)\n",
    "df.columns = [\"file\", \"class\", \"type\", \"breed\"]\n",
    "df.head()\n",
    "breed_regex = '([a-zA-Z_])+(?=_\\d+)' #Regex to extract breed name from file name\n",
    "df[\"breed_name\"] = df[\"file\"].apply(lambda x: re.search(breed_regex,x).group())\n",
    "class_lst = list(df[\"breed_name\"].unique())\n",
    "class_lst\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2944\n",
      "736\n",
      "3669\n"
     ]
    }
   ],
   "source": [
    "trainval_df = pd.read_csv(f\"{m_path}/annotations/trainval.txt\", sep = \" \", header=None)\n",
    "trainval_df.columns = [\"file\", \"class\", \"type\", \"breed\"]\n",
    "\n",
    "X = trainval_df[\"file\"].apply(lambda x: f\"{x}.jpg\")\n",
    "y = trainval_df[\"class\"] - 1\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)      #Split into train val datasets\n",
    "train_df = pd.concat([X_train, y_train], axis = 1)\n",
    "val_df   = pd.concat([X_val, y_val], axis = 1)\n",
    "\n",
    "test_df = pd.read_csv(f\"{m_path}/annotations/test.txt\", sep = \" \", header=None)\n",
    "test_df.columns = [\"file\", \"class\", \"type\", \"breed\"]\n",
    "test_df[\"file\"] = test_df[\"file\"].apply(lambda x: f\"{x}.jpg\")\n",
    "test_df[\"class\"] = test_df[\"class\"] - 1\n",
    "test_df = test_df[[\"file\", \"class\"]]\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to json file for FiftyOne\n",
    "\n",
    "# train_dict = {\"classes\": class_lst}\n",
    "# train_dict[\"labels\"] = {x[1][\"file\"][:-4]:x[1][\"class\"] for x in train_df.iterrows()}\n",
    "# train_json = json.dumps(train_dict, indent=4)\n",
    "# with open(\"train.json\", \"w\") as outfile:\n",
    "#      outfile.write(train_json)\n",
    "     \n",
    "# val_dict   = {\"classes\": class_lst}\n",
    "# val_dict[\"labels\"] = {x[1][\"file\"][:-4]:x[1][\"class\"] for x in val_df.iterrows()}\n",
    "# val_json = json.dumps(val_dict, indent=4)\n",
    "# with open(\"val.json\", \"w\") as outfile:\n",
    "#      outfile.write(val_json)\n",
    "     \n",
    "# test_dict  = {\"classes\": class_lst}\n",
    "# test_dict[\"labels\"] = {x[1][\"file\"][:-4]:x[1][\"class\"] for x in test_df.iterrows()}\n",
    "# test_json = json.dumps(test_dict, indent=4)\n",
    "# with open(\"test.json\", \"w\") as outfile:\n",
    "#      outfile.write(test_json)\n",
    "     \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "Summary:\n",
    "- Defined transformation class to augment data\n",
    "- Finetune ResNet50 to breed classification problem\n",
    "- Use FiftyOne to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pl_bolts/callbacks/data_monitor.py:20: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"wandb\")\n",
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pl_bolts/losses/self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pl_bolts/datamodules/experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n",
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from typing import Callable, Tuple, Union\n",
    "from flash.core.data.transforms import ApplyToKeys\n",
    "from flash.core.data.io.input_transform import InputTransform\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ICDTransform(InputTransform):\n",
    "    image_size: Tuple[int, int] = (300, 300)\n",
    "    mean: Union[float, Tuple[float, float, float]] = (0.485, 0.456, 0.406)\n",
    "    std: Union[float, Tuple[float, float, float]] = (0.229, 0.224, 0.225)\n",
    "    \n",
    "    def per_sample_transform(self):\n",
    "        return T.Compose([\n",
    "            ApplyToKeys(\n",
    "                \"input\",\n",
    "                T.Compose([\n",
    "                    T.ToTensor(),\n",
    "                    T.Resize(self.image_size),\n",
    "                    T.Normalize(self.mean, self.std),\n",
    "                    T.RandomHorizontalFlip(),\n",
    "                    T.ColorJitter(),\n",
    "                    T.RandomPerspective()\n",
    "                ])\n",
    "            ),\n",
    "            ApplyToKeys(\"target\", torch.as_tensor)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import fiftyone as fo\n",
    "from flash.core.classification import FiftyOneLabelsOutput as FOLO\n",
    "from flash.image.classification.data import ImageClassificationData as ICD\n",
    "from flash.core.integrations.fiftyone import visualize\n",
    "from flash.image.classification.model import ImageClassifier as IC\n",
    "from flash import Trainer\n",
    "\n",
    "data_dir = f\"{m_path}/images/\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "trainer = Trainer(\n",
    "                       accelerator=\"gpu\"\n",
    "                     ,max_epochs=5\n",
    "                     ,devices=[0]       #This argument refers to the cuda device\n",
    "                     ,progress_bar_refresh_rate=1\n",
    "                     ,auto_lr_find=True\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 2944/2944 [2.4s elapsed, 0s remaining, 1.3K samples/s]       \n",
      " 100% |█████████████████| 736/736 [473.5ms elapsed, 0s remaining, 1.6K samples/s]      \n",
      " 100% |███████████████| 3669/3669 [2.9s elapsed, 0s remaining, 1.2K samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/flash/core/data/utilities/paths.py:176: UserWarning: Found invalid file extensions: .mat. Files with these extensions will be ignored. The supported file extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp, .npy.\n",
      "  rank_zero_warn(\n",
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3508: FutureWarning: Please pass an instantiated object of the `InputTransform` class. Passing the Class and keyword arguments separately has been deprecated since v0.8.0 and will be removed in v0.9.0.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name          | Type           | Params\n",
      "-------------------------------------------------\n",
      "0 | train_metrics | ModuleDict     | 0     \n",
      "1 | val_metrics   | ModuleDict     | 0     \n",
      "2 | test_metrics  | ModuleDict     | 0     \n",
      "3 | adapter       | DefaultAdapter | 23.6 M\n",
      "-------------------------------------------------\n",
      "128 K     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.6 M    Total params\n",
      "94.335    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:876: UserWarning: Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\n",
      "  warnings.warn(\"Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/114 [00:00<?, ?it/s, loss=0.592, v_num=25, train_accuracy_step=0.719, train_cross_entropy_step=0.767, val_accuracy=0.908, val_cross_entropy=0.420]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/callbacks/finetuning.py:207: UserWarning: The provided params to be frozen already exist within another group of this optimizer. Those parameters will be skipped.\n",
      "HINT: Did you init your optimizer in `configure_optimizer` as such:\n",
      " <class 'torch.optim.adam.Adam'>(filter(lambda p: p.requires_grad, self.parameters()), ...) \n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 114/114 [01:26<00:00,  1.31it/s, loss=0.941, v_num=25, train_accuracy_step=0.594, train_cross_entropy_step=1.230, val_accuracy=0.514, val_cross_entropy=1.850, train_accuracy_epoch=0.607, train_cross_entropy_epoch=1.250]\n"
     ]
    }
   ],
   "source": [
    "#NOTE: THIS CELL IS DEDICATED FOR FINETUNING RESNET50, IGNORE AND RUN NEXT CELL\n",
    "\n",
    "train_dataset = fo.Dataset.from_dir(\n",
    "  dataset_type=fo.types.FiftyOneImageClassificationDataset,\n",
    "  data_path=data_dir,\n",
    "  labels_path=\"./train.json\"\n",
    ")\n",
    "\n",
    "val_dataset = fo.Dataset.from_dir(\n",
    "  dataset_type=fo.types.FiftyOneImageClassificationDataset,\n",
    "  data_path=data_dir,\n",
    "  labels_path=\"./val.json\"\n",
    ")\n",
    "\n",
    "test_dataset = fo.Dataset.from_dir(\n",
    "  dataset_type=fo.types.FiftyOneImageClassificationDataset,\n",
    "  data_path=data_dir,\n",
    "  labels_path=\"./test.json\"\n",
    ")\n",
    "\n",
    "\n",
    "datamodule = ICD.from_fiftyone(\n",
    "  train_dataset=train_dataset,\n",
    "  val_dataset=val_dataset,\n",
    "  test_dataset=test_dataset,\n",
    "  transform=ICDTransform,\n",
    "  transform_kwargs=dict(image_size=(300, 300)),\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "model = IC(backbone='resnet50', num_classes=datamodule.num_classes)\n",
    "\n",
    "                     \n",
    "trainer.finetune(model, datamodule=datamodule, strategy=(\"freeze_unfreeze\", 1))\n",
    "trainer.save_checkpoint(\"ResNet50_LightningFLash_OxfordPet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 3669/3669 [3.3s elapsed, 0s remaining, 1.0K samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting:  80%|████████  | 92/115 [00:00<00:06,  3.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/flash/core/classification.py:284: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = torch.tensor(pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 115/115 [00:34<00:00,  1.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6008/?notebook=True&subscription=c2d8d3e7-a710-4c3b-bba3-309db2fa1099\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdfa7938c70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook sessions cannot wait\n"
     ]
    }
   ],
   "source": [
    "test_dataset = fo.Dataset.from_dir(\n",
    "  dataset_type=fo.types.FiftyOneImageClassificationDataset,\n",
    "  data_path=data_dir,\n",
    "  labels_path=\"./test.json\"\n",
    ")\n",
    "\n",
    "model = IC.load_from_checkpoint(\"ResNet50_LightningFLash_OxfordPet.pt\")\n",
    "datamodule = ICD.from_fiftyone(\n",
    "  predict_dataset=test_dataset,\n",
    "  batch_size = BATCH_SIZE\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(model, datamodule=datamodule, output=FOLO(class_lst, return_filepath=False))\n",
    "predictions = list(chain.from_iterable(predictions))\n",
    "test_dataset.set_values(\"predictions\", predictions)\n",
    "results = test_dataset.evaluate_classifications(\"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\")\n",
    "\n",
    "session = fo.launch_app(test_dataset, port=6008)    #Open localhost:6008 on any browser to interact with FiftyOne\n",
    "session.wait()\n",
    "# session.close()    #Execute this code to close FiftyOne session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datnt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
